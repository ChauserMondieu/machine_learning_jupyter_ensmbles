{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归 Linear Regression\n",
    "## 多元线性回归\n",
    "多元线性回归是回归分析的一种，所谓回归分析，就是由已知的自变量估计因变量的条件期望\n",
    "线性回归的模型使用矩阵表述如下所示\n",
    "$$\n",
    "h_{\\theta}(x_0,x_1,...,x_n) = {\\sum}_{i=0}^{n}\\theta_ix_i\n",
    "$$\n",
    "其中 $n$ 代表了自变量的数量，换言之就是属性量，如果同时输入多个样本，我们采用 $m$ 代表样本数量，那么输入自变量可以使用一个 $X_{m*n}$ 矩阵来表示：\n",
    "$$\n",
    "h_{\\theta}(X) = X \\cdot \\theta\n",
    "$$\n",
    "当采用 $MSE$ 均方误差作为损失函数时，通过矩阵形式表示为：\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2}(X\\theta - Y)^T(X\\theta - Y)\n",
    "$$\n",
    "此时，有两种方法能够求解出变量矩阵 $\\theta$ </br>\n",
    "<ul>\n",
    "1. 首先是解析法，代表就是最小二乘法 $LSM$ ，由于损失函数是凸函数因此求该函数的一阶导数，令一阶导数为0即可求出相应值，于是解析解答案使用矩阵表示为\n",
    "$$\n",
    "\\theta = (X^TX)^{-1}X^TY\n",
    "$$\n",
    "</ul>\n",
    "<ul>\n",
    "2. 其次是数值法，代表就是梯度下降法 $GD$ ，由于通过迭代求出最优 $\\theta$ 矩阵，每一次更新的函数通过矩阵表示为\n",
    "$$\n",
    "\\theta = \\theta - \\alpha X^T(\\theta X-Y)\n",
    "$$\n",
    "其中 $\\alpha$ 就是学习速率，也就是通常所说的步长， $X^T(\\theta X-Y)$ 就是损失函数的 Jacobian 矩阵\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
